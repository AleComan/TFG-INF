{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7ba9775",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# **Alejandro Coman Venceslá**\n",
    "\n",
    "### Doble Grado en Ingeniería Informática y  \n",
    "### Administración y Dirección de Empresas\n",
    "#### Universidad de Granada\n",
    "\n",
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://etsiit.ugr.es/sites/centros/etsiit/public/template-extra/etsiit-logo.png\" alt=\"Imagen 1\" style=\"width: 200px; margin-right: 40px;\">\n",
    "  <img src=\"https://etsiit.ugr.es/sites/centros/etsiit/public/color/ugr-41cc9222/logo-mono.svg\" alt=\"Imagen 2\" style=\"width: 300px; margin-left: 40px; margin-bottom: 60px\">\n",
    "</div>\n",
    "\n",
    "**Trabajo de Fin de Grado**\n",
    "\n",
    "<br><br>\n",
    "\n",
    "*Análisis de sesgos en modelos de inteligencia artificial generativa textual.*\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09fe3d6",
   "metadata": {},
   "source": [
    "# Capítulo 2.2. Extracción de datos de las biografías\n",
    "\n",
    "Este cuaderno contiene el código empleado para el primer capítulo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "074cc6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Para TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Para sentiment analysis\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# 1. Calcula la carpeta padre del notebook\n",
    "parent_dir = Path().resolve().parent\n",
    "\n",
    "# 2. Inserta esa ruta al principio de sys.path\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "# 3. Ahora ya puedes importar\n",
    "from variables import *\n",
    "\n",
    "BIO_CSV_PATH = 'biografias_raw_corrected.csv'\n",
    "FREQ_CSV_PATH = '../Capítulo primero/frecuentes_corrected.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "959e9078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aleja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aleja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\aleja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\aleja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Asumimos que el CSV biografias_raw.csv ya tiene encabezado en la primera fila:\n",
    "df_bio = pd.read_csv(BIO_CSV_PATH, encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "# Leer el CSV de información adicional de los personajes\n",
    "df_freq = pd.read_csv(FREQ_CSV_PATH, encoding='utf-8').drop(columns=[\"Frequency\"])\n",
    "\n",
    "# Identificar columnas de modelos (todas menos \"Name\")\n",
    "model_columns = [col for col in df_bio.columns if col.lower() != \"name\"]\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# 5. Inicializar el analizador VADER\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd2176ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para obtener top-N keywords de un vector TF-IDF\n",
    "def get_top_n_keywords(row_tfidf, feature_names, n=5):\n",
    "\t# row_tfidf es un vector escaso (sparse) de TF-IDF para un documento\n",
    "\trow_array = row_tfidf.toarray().flatten()\n",
    "\tif np.all(row_array == 0):\n",
    "\t\treturn []\n",
    "\ttop_n_idx = row_array.argsort()[-n:][::-1]\n",
    "\treturn feature_names[top_n_idx].tolist()\n",
    "\n",
    "# 5. Función para sanitizar nombres de modelo y que sean válidos como filenames\n",
    "def sanitize_model_name(name: str) -> str:\n",
    "    # Reemplaza cualquier caracter no alfanumérico (excepto guiones bajos o puntos) por guión bajo\n",
    "    safe = re.sub(r\"[^A-Za-z0-9\\-_\\.]\", \"_\", name)\n",
    "    return safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "636d2cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Resultados para modelo 'openai/gpt-4o-mini' guardados en: openai_gpt-4o-mini.csv\n",
      ">> Resultados para modelo 'deepseek/deepseek-chat-v3-0324' guardados en: deepseek_deepseek-chat-v3-0324.csv\n",
      ">> Resultados para modelo 'google/gemini-2.0-flash-001' guardados en: google_gemini-2.0-flash-001.csv\n",
      ">> Resultados para modelo 'microsoft/phi-4-multimodal-instruct' guardados en: microsoft_phi-4-multimodal-instruct.csv\n",
      ">> Resultados para modelo 'meta-llama/llama-4-maverick' guardados en: meta-llama_llama-4-maverick.csv\n"
     ]
    }
   ],
   "source": [
    "for model_name in model_columns:\n",
    "    \n",
    "\t# 7.1. Sanitizamos el nombre para el CSV de salida\n",
    "    modelo_seguro = sanitize_model_name(model_name)\n",
    "    \n",
    "    # 7.2. Comprobamos que exista esa columna en df_bio\n",
    "    if model_name not in df_bio.columns:\n",
    "        print(f\">> Advertencia: la columna '{model_name}' no existe en biografias_raw.csv. Se salta este modelo.\")\n",
    "        continue\n",
    "    \n",
    "    # Extraer solo la columna de nombre y la columna de ese modelo\n",
    "    temp = df_bio[[\"Name\", model_name]].copy()\n",
    "    temp = temp.rename(columns={model_name: \"biography\"})\n",
    "    \n",
    "    # Preprocesamiento mínimo: rellenar nulos con cadena vacía\n",
    "    temp[\"biography\"] = temp[\"biography\"].fillna(\"\")\n",
    "\n",
    "    # --------------- TF-IDF para extraer Top-5 palabras clave ---------------\n",
    "    # Construimos un corpus con todas las biografías de este modelo\n",
    "    corpus = temp[\"biography\"].tolist()\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,1))\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)  # matriz (n_docs x n_vocab)\n",
    "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "    \n",
    "\t# Contenedor para los resultados\n",
    "    results = {\n",
    "        \"Name\": [],\n",
    "        \"top5_keywords\": [],\n",
    "        \"sent_neg\": [],\n",
    "        \"sent_neu\": [],\n",
    "        \"sent_pos\": [],\n",
    "        \"sent_compound\": [],\n",
    "        \"polarity\": [],\n",
    "        \"subjectivity\": []\n",
    "    }\n",
    "    \n",
    "\t# Iterar por cada fila/documento\n",
    "    for idx, row in temp.iterrows():\n",
    "        name = row[\"Name\"]\n",
    "        text = row[\"biography\"]\n",
    "        \n",
    "        # Extraer Top-5 keywords\n",
    "        tfidf_vector = tfidf_matrix[idx]\n",
    "        top5 = get_top_n_keywords(tfidf_vector, feature_names, n=5)\n",
    "        \n",
    "        # Sentimiento con VADER\n",
    "        vs = sia.polarity_scores(text)\n",
    "        # vs devuelve diccionario con keys ['neg','neu','pos','compound']\n",
    "        \n",
    "        # Polarity y subjectivity con TextBlob\n",
    "        tb = TextBlob(text)\n",
    "        pol = tb.sentiment.polarity\n",
    "        subj= tb.sentiment.subjectivity\n",
    "        \n",
    "        # Guardar en listas\n",
    "        results[\"Name\"].append(name)\n",
    "        results[\"top5_keywords\"].append(\", \".join(top5))   # guardamos como string separado por comas\n",
    "        results[\"sent_neg\"].append(vs[\"neg\"])\n",
    "        results[\"sent_neu\"].append(vs[\"neu\"])\n",
    "        results[\"sent_pos\"].append(vs[\"pos\"])\n",
    "        results[\"sent_compound\"].append(vs[\"compound\"])\n",
    "        results[\"polarity\"].append(pol)\n",
    "        results[\"subjectivity\"].append(subj)\n",
    "    \n",
    "    # Crear DataFrame de resultados de este modelo\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_merged = pd.merge(df_results, df_freq, on=\"Name\", how=\"left\")\n",
    "    \n",
    "\t# reordenar columnas\n",
    "    demographic_cols = [c for c in df_freq.columns if c != \"Name\"]\n",
    "    metric_cols      = [\n",
    "        \"top5_keywords\",\n",
    "        \"sent_neg\",\n",
    "        \"sent_neu\",\n",
    "        \"sent_pos\",\n",
    "        \"sent_compound\",\n",
    "        \"polarity\",\n",
    "        \"subjectivity\"\n",
    "    ]\n",
    "    new_column_order = [\"Name\"] + demographic_cols + metric_cols\n",
    "    df_merged = df_merged[new_column_order]\n",
    "\t\n",
    "    # Unir con la info demográfica de df_freq (opcional, pero útil para ver sesgos por género, país, etc.)\n",
    "    # Nota: asumimos que df_freq tiene una columna \"Name\" compatible\n",
    "    \n",
    "    fname = f\"{modelo_seguro}.csv\"\n",
    "    df_merged.to_csv(fname, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    print(f\">> Resultados para modelo '{model_name}' guardados en: {fname}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
